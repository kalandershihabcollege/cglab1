{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KalanderLab2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GREK5DUFgUb5"
      },
      "source": [
        "#libraries \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFcCSoMphGac"
      },
      "source": [
        "dataset = pd.read_csv('zoo.csv',\n",
        "                      names=['animal_name','hair','feathers','eggs','milk',\n",
        "                                                   'airbone','aquatic','predator','toothed','backbone',\n",
        "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qk09C0ZXhT6B"
      },
      "source": [
        "# Predicting the type of animal based on thier names is irrelavant, so reject name as feature\n",
        "\n",
        "dataset=dataset.drop('animal_name',axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVLrhdJriTva"
      },
      "source": [
        "**Decision tree using ID3 algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY6mvZSgiY8I"
      },
      "source": [
        "#calculate entropy\n",
        "def entropy(target_col):\n",
        "    elements,counts = np.unique(target_col,return_counts = True)\n",
        "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
        "    return entropy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztLRk2sOidMQ"
      },
      "source": [
        "#information gain\n",
        "def InfoGain(data,split_attribute_name,target_name=\"class\"):    \n",
        "    total_entropy = entropy(data[target_name]) \n",
        "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
        "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
        "    Information_Gain = total_entropy - Weighted_Entropy\n",
        "    return Information_Gain"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCV3PoB1ihO6"
      },
      "source": [
        "#to build decision tree\n",
        "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n",
        "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
        "        return np.unique(data[target_attribute_name])[0]\n",
        "    \n",
        "    elif len(data)==0:\n",
        "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
        "    \n",
        "    elif len(features) ==0:\n",
        "        return parent_node_class\n",
        "    \n",
        "    else:\n",
        "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
        "        \n",
        "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features]\n",
        "        best_feature_index = np.argmax(item_values)\n",
        "        best_feature = features[best_feature_index]\n",
        "        tree = {best_feature:{}}\n",
        "        \n",
        "        features = [i for i in features if i != best_feature]\n",
        "        \n",
        "        for value in np.unique(data[best_feature]):\n",
        "            value = value\n",
        "            sub_data = data.where(data[best_feature] == value).dropna()\n",
        "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
        "            tree[best_feature][value] = subtree\n",
        "            \n",
        "        return(tree)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA7JXizjil9s"
      },
      "source": [
        "def predict(query,tree,default = 1):\n",
        "    for key in list(query.keys()):\n",
        "        if key in list(tree.keys()):\n",
        "            try:\n",
        "                result = tree[key][query[key]] \n",
        "            except:\n",
        "                return default\n",
        "  \n",
        "            result = tree[key][query[key]]\n",
        "            if isinstance(result,dict):\n",
        "                return predict(query,result)\n",
        "\n",
        "            else:\n",
        "                return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOD3rT-uivYo"
      },
      "source": [
        "**Dataset Spliting and Predicting and test accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2_X07PNiwn1"
      },
      "source": [
        "def train_test_split(dataset):\n",
        "    training_data = dataset.iloc[:80].reset_index(drop=True)\n",
        "    testing_data = dataset.iloc[80:].reset_index(drop=True)\n",
        "    return training_data,testing_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EskUMSAnizbW"
      },
      "source": [
        "training_data = train_test_split(dataset)[0]\n",
        "testing_data = train_test_split(dataset)[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPjwXsd6i1-A"
      },
      "source": [
        "def test(data,tree):\n",
        "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
        "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
        "    \n",
        "    for i in range(len(data)):\n",
        "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n",
        "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7vStLDdi497",
        "outputId": "07e37847-54ef-4c3b-b295-e01de0d8b65d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tree = ID3(training_data,training_data,training_data.columns[:-1])\n",
        "pprint(tree)\n",
        "test(testing_data,tree)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'legs': {'0': {'fins': {'0': {'toothed': {'0': '7', '1': '3'}},\n",
            "                         '1': {'eggs': {'0': '1', '1': '4'}}}},\n",
            "          '2': {'hair': {'0': '2', '1': '1'}},\n",
            "          '4': {'hair': {'0': {'toothed': {'0': '7', '1': '5'}}, '1': '1'}},\n",
            "          '6': {'aquatic': {'0': '6', '1': '7'}},\n",
            "          '8': '7',\n",
            "          'legs': 'class'}}\n",
            "The prediction accuracy is:  86.36363636363636 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}